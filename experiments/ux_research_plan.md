# NM Script Editor UX Research Plan

**Issue**: #247 - Conduct Comprehensive UX Study with Real Users
**Date**: 2026-01-08
**Status**: Research Protocol - Ready for Execution
**Version**: 1.0

---

## Executive Summary

This document outlines a comprehensive user experience research study to validate the technical analysis conducted in issue #227 and identify pain points through empirical data from real users. The study employs a mixed-methods approach combining qualitative interviews, quantitative usability testing, large-scale surveys, and ongoing analytics.

**Research Question**: Do the 42 UI/UX issues identified through technical analysis align with actual user pain points, and what critical issues were missed?

**Timeline**: 6 weeks for initial study + ongoing analytics
**Target Participants**: 65-70 users across all experience levels
**Budget**: TBD (depends on participant incentives)

---

## 1. Background & Rationale

### 1.1 Prior Technical Analysis

The technical analysis (issue #227 + extensions) identified **42 UI/UX issues** based on:
- Code inspection of `editor/script_editor/` components
- Architecture review of editor-graph-preview integration
- Comparison with modern editors (VS Code, Sublime, JetBrains)
- Best practices from HCI literature

**Technical Analysis Rating**: 7.5/10 UI/UX quality

**Issues Categorized**:
- ğŸ”´ High Priority: 14 issues (e.g., #237-#242)
- ğŸŸ¡ Medium Priority: 18 issues (e.g., #243-#244)
- ğŸŸ¢ Low Priority: 10 issues (accessibility, polish)

**Key Findings from Technical Analysis**:
1. **Feature Discoverability**: Advanced features hidden (snippets, command palette, quick fixes)
2. **Integration Gaps**: Script â†” Graph bidirectional navigation missing
3. **Onboarding**: No welcome dialog or tutorial for new users
4. **Live Preview**: No split-view mode for scene preview while editing
5. **Customization**: Settings not persistent, no theme options

### 1.2 Limitations of Technical Analysis

While comprehensive, technical analysis has blind spots:
- âŒ **Cannot predict actual user behavior** (expected vs. observed workflows)
- âŒ **Cannot measure task completion time** (theoretical vs. empirical efficiency)
- âŒ **Cannot identify "unknown unknowns"** (issues we didn't think to look for)
- âŒ **Cannot validate assumptions** (e.g., "users will discover command palette")
- âŒ **Cannot measure emotional response** (frustration, delight, confusion)

**Real user research is essential** to:
1. Validate which technical issues actually impact users
2. Discover pain points invisible to code inspection
3. Prioritize fixes based on observed severity (not assumed)
4. Understand mental models and workflow patterns
5. Measure baseline metrics for future comparison

---

## 2. Research Objectives

### 2.1 Primary Objectives

1. **Validate Technical Findings**: Do users experience the 42 identified issues?
2. **Discover Missed Issues**: What pain points did technical analysis miss?
3. **Prioritize Fixes**: Which issues have highest real-world impact?
4. **Baseline Metrics**: Establish current performance benchmarks

### 2.2 Secondary Objectives

5. **Understand Workflows**: How do different user segments use the editor?
6. **Feature Awareness**: What features do users discover vs. miss?
7. **Competitive Positioning**: How does NM Script compare to Ren'Py, VN Maker?
8. **Satisfaction Drivers**: What factors most influence user satisfaction?

### 2.3 Research Questions

**Discoverability**:
- RQ1: Do users discover the Command Palette within first session?
- RQ2: What percentage of users know about snippet insertion (Ctrl+J)?
- RQ3: Are users aware of bidirectional navigation to Story Graph?

**Usability**:
- RQ4: Can beginners create a simple 3-scene script without help?
- RQ5: What is the task completion rate for common workflows?
- RQ6: Where do users get stuck or make errors?

**Integration**:
- RQ7: Do users understand the relationship between Script Editor, Story Graph, and Scene Preview?
- RQ8: How often do users need to switch between panels?
- RQ9: Do users expect real-time preview while editing?

**Pain Points**:
- RQ10: What tasks are frustrating or slow for users?
- RQ11: What features do users expect but cannot find?
- RQ12: What error messages confuse users?

---

## 3. Study Design Overview

### 3.1 Mixed-Methods Approach

This study employs **triangulation** - using multiple methods to cross-validate findings:

```
Phase 1: Qualitative Interviews (Deep insights, small N)
    â†“
Phase 2: Quantitative Usability Tests (Metrics, medium N)
    â†“
Phase 3: Survey (Broad patterns, large N)
    â†“
Phase 4: Analytics (Continuous monitoring, all users)
```

### 3.2 Participant Recruitment

**Target Sample**: 65-70 participants total

**Segmentation by Experience**:
- **Beginners** (40%): Never used NM Script Editor before
- **Intermediate** (40%): Used for 1-3 projects
- **Advanced** (20%): Power users, 4+ projects

**Segmentation by Role** (if applicable):
- Writers/Narrative Designers
- Programmers/Developers
- Hobbyists/Independent Creators
- Students/Learners

**Recruitment Channels**:
1. In-app recruitment prompt (for existing users)
2. GitHub issue comment / Discussions post
3. Discord / Community forums (if available)
4. Social media (Twitter, Reddit r/visualnovels)
5. Visual novel developer communities

**Incentives**:
- **Interviews**: $50 gift card or equivalent (1 hour commitment)
- **Usability Tests**: $30 gift card (45 minutes commitment)
- **Survey**: Entry into raffle for $100 prize (5 minutes)
- **Analytics**: Opt-in, no incentive (passive)

**Screening Criteria**:
- Must have interest in visual novel development
- Must have access to computer with editor installed
- Beginners: No prior VN development experience OK
- Intermediate/Advanced: Portfolio or project files requested

---

## 4. Phase 1: User Interviews (Weeks 1-2)

### 4.1 Objectives

- Understand users' mental models and expectations
- Identify pain points in natural language (not technical terms)
- Explore workflows and tool usage patterns
- Discover "unknown unknowns" missed by technical analysis

### 4.2 Method: Semi-Structured Interviews

**Format**: 1-on-1 remote video calls (Zoom, Google Meet)
**Duration**: 45-60 minutes per participant
**Sample Size**: 10-15 participants
**Composition**: 5 beginners, 5 intermediate, 3-5 advanced

### 4.3 Interview Protocol

**Introduction (5 minutes)**:
- Explain study purpose and consent
- Assure confidentiality and no "wrong answers"
- Request permission to record (audio/video)

**Background Questions (10 minutes)**:
1. Tell me about your experience with visual novel development
2. What tools have you used before? (Ren'Py, Twine, etc.)
3. What types of stories do you want to create?
4. How did you learn about NM Script Editor?

**Discovery & Learning (10 minutes)**:
5. How did you first learn NM Script? Walk me through that experience.
6. What was confusing or surprising when you started?
7. What documentation or resources did you use?
8. What features did you discover by accident vs. intentionally?
9. If you got stuck, how did you get unstuck?

**Workflow & Tasks (15 minutes)**:
10. Walk me through creating a typical scene from start to finish (think-aloud protocol)
11. Show me how you navigate between different parts of your project
12. How do you test/preview your scenes?
13. How do you handle errors or bugs in your scripts?
14. What's the most time-consuming part of your workflow?

**Pain Points (10 minutes)**:
15. What tasks are frustrating or slow?
16. What have you tried to do but couldn't figure out?
17. What features do you wish existed?
18. If you could change one thing about the editor, what would it be?

**Comparison (5 minutes)**:
19. How does NM Script compare to other tools you've used?
20. What does it do better? What does it do worse?

**Closing (5 minutes)**:
21. Is there anything else you'd like to share?
22. Any questions for me?

### 4.4 Data Collection & Analysis

**Recording**:
- Audio/video recordings (with consent)
- Interviewer notes during session
- Screen recordings if participant does demo

**Analysis Method**: Thematic Analysis
1. Transcribe all interviews (automated + human review)
2. Code transcripts for themes (open coding)
3. Group codes into categories (axial coding)
4. Identify patterns across participants (selective coding)
5. Map themes to technical analysis issues

**Deliverable**: Interview Summary Report (Week 2)
- Key themes and quotes
- Pain point frequency analysis
- Comparison with technical findings
- Recommendations for usability test scenarios

---

## 5. Phase 2: Usability Testing (Weeks 3-4)

### 5.1 Objectives

- Measure task completion rates objectively
- Identify specific friction points in workflows
- Quantify time-on-task for common operations
- Observe errors and confusion points

### 5.2 Method: Task-Based Usability Testing

**Format**: Remote moderated sessions
**Duration**: 45 minutes per participant
**Sample Size**: 15-20 participants
**Composition**: 8 beginners, 7 intermediate, 3-5 advanced

### 5.3 Test Scenarios & Tasks

Participants complete 6 standardized tasks while thinking aloud:

#### Task 1: Create a Simple Script (Beginner)
**Scenario**: "You're creating your first visual novel scene. Create a script with 3 characters having a conversation, then transition to another scene."

**Success Criteria**:
- âœ… Create new script file
- âœ… Define 3 character declarations
- âœ… Write dialogue with at least 3 exchanges
- âœ… Add a `goto` to transition to next scene
- âœ… Save file without errors

**Measured Metrics**:
- Task completion rate (%)
- Time to completion (minutes)
- Number of errors / false starts
- Help requests
- Subjective difficulty (1-5 scale)

#### Task 2: Add a Choice Branch (Intermediate)
**Scenario**: "Add a choice to your scene where the player decides between two dialogue options, leading to different outcomes."

**Success Criteria**:
- âœ… Insert `choice` statement with 2 options
- âœ… Create consequence branches
- âœ… Preview works correctly

**Measured Metrics**:
- Same as Task 1
- Specific: Did user find `choice` syntax in documentation/snippets?

#### Task 3: Fix a Syntax Error (All Levels)
**Scenario**: "This script has an error. Find and fix it."
(Pre-seeded error: undefined scene reference)

**Success Criteria**:
- âœ… Identify error location from editor indicators
- âœ… Understand error message
- âœ… Apply correct fix
- âœ… Verify error cleared

**Measured Metrics**:
- Time to identify error
- Time to fix error
- Correct fix on first attempt (%)
- Error message comprehension (yes/no)

#### Task 4: Navigate to Scene Definition (Intermediate/Advanced)
**Scenario**: "Your script calls `goto chapter2_intro`. Jump to that scene's definition."

**Success Criteria**:
- âœ… Use F12 (Go to Definition) feature
- OR âœ… Use Ctrl+Shift+G (Navigate to Graph)
- OR âœ… Manually find scene in file tree

**Measured Metrics**:
- Method used (F12 / Graph / Manual)
- Time to find definition
- Did user know about F12 feature? (yes/no)

#### Task 5: Insert a Snippet (Advanced Feature Discovery)
**Scenario**: "Add a character sprite to your scene using the editor's snippet feature."

**Success Criteria**:
- âœ… Discover Ctrl+J snippet insertion
- OR âœ… Discover Insert menu
- âœ… Select and insert `show` command snippet
- âœ… Fill in placeholders

**Measured Metrics**:
- Did user discover snippets without prompting? (yes/no)
- Method of discovery (keyboard shortcut / menu / gave up)
- Time to insert snippet

#### Task 6: Use Command Palette (Advanced Feature Discovery)
**Scenario**: "Use the Command Palette to format your code."

**Success Criteria**:
- âœ… Open Command Palette (Ctrl+Shift+P)
- âœ… Search for "format" command
- âœ… Execute format command

**Measured Metrics**:
- Did user know Command Palette exists? (yes/no)
- Time to complete (if successful)
- Discovery method if not prompted

### 5.4 Observation Protocol

**Think-Aloud Protocol**:
- Participants verbalize thoughts while completing tasks
- Moderator prompts: "What are you thinking?", "What do you expect to happen?"
- Avoid leading questions or hints

**Data Collection**:
- Screen recording + audio
- Moderator notes on confusion points
- Automated time tracking (task start/complete timestamps)
- Post-task questionnaire (Single Ease Question: "How difficult was this task?" 1-5)

### 5.5 Analysis

**Quantitative Metrics**:
| Metric | Target | Minimum Acceptable |
|--------|--------|-------------------|
| Task 1 Completion | 90% | 75% |
| Task 2 Completion | 80% | 65% |
| Task 3 Completion | 85% | 70% |
| Task 4 (F12 used) | 60% | 40% |
| Task 5 (Snippets) | 50% | 30% |
| Task 6 (Cmd Palette) | 40% | 20% |

**Qualitative Analysis**:
- Confusion point heatmap (where users hesitate)
- Error pattern analysis (common mistakes)
- Feature discovery rates
- Correlation with experience level

**Deliverable**: Usability Test Report (Week 4)
- Task completion statistics
- Time-on-task analysis
- Heatmaps of confusion points
- Video highlights of critical incidents
- Prioritized list of top 10 friction points

---

## 6. Phase 3: Survey (Week 5)

### 6.1 Objectives

- Reach broad user base (50+ responses)
- Validate patterns found in interviews/usability tests
- Measure feature awareness at scale
- Assess satisfaction and Net Promoter Score

### 6.2 Method: Online Questionnaire

**Format**: Online survey (Google Forms / Typeform / Qualtrics)
**Duration**: 5-7 minutes
**Sample Size**: Target 50+ responses (no maximum)
**Distribution**: In-app prompt, GitHub, Discord, email list

### 6.3 Survey Sections

#### Section 1: Demographics & Experience (5 questions)
1. How long have you been using NM Script Editor?
   - [ ] Less than 1 month
   - [ ] 1-3 months
   - [ ] 3-6 months
   - [ ] 6-12 months
   - [ ] More than 1 year

2. How many projects have you created with NM Script?
   - [ ] 0 (just exploring)
   - [ ] 1-2
   - [ ] 3-5
   - [ ] 6-10
   - [ ] More than 10

3. What is your primary role? (Select all that apply)
   - [ ] Writer / Narrative Designer
   - [ ] Programmer / Developer
   - [ ] Artist / Visual Designer
   - [ ] Sound Designer / Composer
   - [ ] Hobbyist / Independent Creator
   - [ ] Student / Learner
   - [ ] Other: __________

4. What other VN tools have you used? (Select all that apply)
   - [ ] Ren'Py
   - [ ] VN Maker
   - [ ] Twine
   - [ ] Unity + VN plugin
   - [ ] Other: __________
   - [ ] None (NM Script is my first)

5. How did you learn NM Script?
   - [ ] Official documentation
   - [ ] Tutorial videos
   - [ ] Sample projects / examples
   - [ ] Trial and error
   - [ ] Asked in community / forum
   - [ ] Other: __________

#### Section 2: Feature Awareness (10 questions)

*Instructions: Mark which features you're aware of and have used*

6. Are you aware of the **Command Palette** (Ctrl+Shift+P)?
   - [ ] Never heard of it
   - [ ] Heard of it but never used
   - [ ] Use it occasionally
   - [ ] Use it frequently

7. Are you aware of **Snippet Insertion** (Ctrl+J)?
   - [ ] Never heard of it
   - [ ] Heard of it but never used
   - [ ] Use it occasionally
   - [ ] Use it frequently

8. Are you aware of **Go to Definition** (F12)?
   - [ ] Never heard of it
   - [ ] Heard of it but never used
   - [ ] Use it occasionally
   - [ ] Use it frequently

9. Are you aware of **Navigate to Story Graph** (Ctrl+Shift+G)?
   - [ ] Never heard of it
   - [ ] Heard of it but never used
   - [ ] Use it occasionally
   - [ ] Use it frequently

10. Are you aware of **Code Folding** (collapse/expand sections)?
    - [ ] Never heard of it
    - [ ] Heard of it but never used
    - [ ] Use it occasionally
    - [ ] Use it frequently

11. Are you aware of the **Minimap**?
    - [ ] Never heard of it
    - [ ] Heard of it but never used
    - [ ] Use it occasionally
    - [ ] Use it frequently

12. Are you aware of **Quick Fixes** (lightbulb icon on errors)?
    - [ ] Never heard of it
    - [ ] Heard of it but never used
    - [ ] Use it occasionally
    - [ ] Use it frequently

13. Are you aware of **Symbol Navigator** (Ctrl+Shift+O)?
    - [ ] Never heard of it
    - [ ] Heard of it but never used
    - [ ] Use it occasionally
    - [ ] Use it frequently

14. Are you aware of **Find References** (Shift+F12)?
    - [ ] Never heard of it
    - [ ] Heard of it but never used
    - [ ] Use it occasionally
    - [ ] Use it frequently

15. How did you discover these advanced features? (Select all that apply)
    - [ ] Documentation
    - [ ] Tooltip / hover hints
    - [ ] Accidentally by keyboard shortcut
    - [ ] Told by another user
    - [ ] Saw in toolbar / menu
    - [ ] Haven't discovered most of them
    - [ ] Other: __________

#### Section 3: Workflow & Productivity (7 questions)

16. How satisfied are you with NM Script Editor overall?
    - [ ] Very dissatisfied
    - [ ] Dissatisfied
    - [ ] Neutral
    - [ ] Satisfied
    - [ ] Very satisfied

17. How likely are you to recommend NM Script to a friend? (Net Promoter Score)
    - 0 (Not at all likely) â†’ 10 (Extremely likely)

18. Which tasks take the most time in your workflow? (Rank top 3)
    - Writing dialogue
    - Creating character definitions
    - Setting up scene structure
    - Debugging errors
    - Testing/previewing scenes
    - Navigating between files/scenes
    - Formatting code
    - Other: __________

19. Which tasks are most frustrating? (Select up to 3)
    - Writing dialogue
    - Creating character definitions
    - Setting up scene structure
    - Debugging errors
    - Testing/previewing scenes
    - Navigating between files/scenes
    - Formatting code
    - Understanding error messages
    - Finding documentation
    - Other: __________

20. How often do you switch between Script Editor and Story Graph?
    - [ ] Rarely / never
    - [ ] Occasionally (a few times per session)
    - [ ] Frequently (many times per session)
    - [ ] Constantly (back and forth)

21. Do you wish you could see Scene Preview while editing scripts?
    - [ ] Yes, I really want this feature
    - [ ] Maybe, it could be useful
    - [ ] No, I don't need it
    - [ ] I don't use Scene Preview

22. How would you rate the **integration** between Script Editor, Story Graph, and Scene Preview?
    - [ ] Very poor (feels disconnected)
    - [ ] Poor (hard to keep in sync)
    - [ ] Fair (works but could be better)
    - [ ] Good (generally smooth)
    - [ ] Excellent (seamless)

#### Section 4: Pain Points & Wishes (5 questions)

23. What is the MOST frustrating aspect of NM Script Editor? (Open-ended)
    _________________________________

24. What is the MOST helpful aspect of NM Script Editor? (Open-ended)
    _________________________________

25. If you could add ONE feature, what would it be? (Open-ended)
    _________________________________

26. Have you encountered errors or bugs? If yes, briefly describe: (Open-ended)
    _________________________________

27. Any other feedback or suggestions? (Open-ended)
    _________________________________

#### Section 5: Performance & Accessibility (3 questions)

28. How would you rate the editor's **performance** (speed, responsiveness)?
    - [ ] Very slow (unusable)
    - [ ] Slow (noticeable lag)
    - [ ] Acceptable
    - [ ] Fast
    - [ ] Very fast

29. Have you experienced performance issues with large scripts?
    - [ ] Yes, frequently
    - [ ] Yes, occasionally
    - [ ] No
    - [ ] I haven't worked with large scripts yet

30. Do you have any accessibility needs that the editor doesn't accommodate? (e.g., screen reader, high contrast, font size)
    - [ ] Yes (please specify): __________
    - [ ] No

### 6.4 Analysis Plan

**Quantitative Analysis**:
- Feature awareness rates (% aware, % using)
- Satisfaction scores (mean, distribution)
- Net Promoter Score calculation
- Task difficulty rankings
- Correlation analysis (e.g., experience vs. feature awareness)

**Qualitative Analysis**:
- Open-ended response coding (same thematic analysis as interviews)
- Word cloud of common terms in pain points
- Sentiment analysis of feedback

**Deliverable**: Survey Analysis Report (Week 5)
- Response rate and demographics
- Feature awareness dashboard
- NPS score and satisfaction metrics
- Top pain points (by frequency)
- Top feature requests (by frequency)
- Comparison with interview/usability test findings

---

## 7. Phase 4: Analytics Telemetry (Ongoing)

### 7.1 Objectives

- Continuously monitor feature usage
- Track error rates and crash patterns
- Identify navigation patterns
- Measure performance metrics at scale

### 7.2 Method: Opt-In Instrumented Analytics

**Implementation**: Add telemetry module to editor
**Opt-In**: Prominent consent dialog on first launch
**Privacy**: Anonymized data only, no PII
**Storage**: Local aggregation â†’ cloud dashboard (secure)

### 7.3 Metrics to Track

#### Usage Metrics
- **Session Duration**: Time editor is open per session
- **Active Time**: Time user is actively editing (keyboard/mouse activity)
- **Files Opened**: Number of script files per session
- **Lines Written**: Lines of code added per session
- **Feature Usage Frequency**:
  - Command Palette opens (Ctrl+Shift+P)
  - Snippet insertions (Ctrl+J)
  - Go to Definition uses (F12)
  - Find References uses (Shift+F12)
  - Navigate to Graph (Ctrl+Shift+G)
  - Format command uses
  - Find/Replace uses
  - Undo/Redo uses

#### Error Metrics
- **Syntax Errors**: Errors per session
- **Error Types**: Distribution of error codes
- **Error Resolution Time**: Time from error appearance to fix
- **Crash Reports**: Frequency and stack traces

#### Performance Metrics
- **Editor Load Time**: Startup time
- **File Open Time**: Time to open script file
- **Syntax Highlight Lag**: Time to colorize large file
- **Memory Usage**: Peak RAM usage per session
- **CPU Usage**: Average CPU during editing

#### Navigation Metrics
- **Panel Switches**: Frequency of switching between panels
- **File Switches**: How often users switch between files
- **Navigation Methods**: F12 vs. manual search vs. Graph navigation
- **Scroll Patterns**: Heatmap of where users spend time in files

### 7.4 Privacy & Ethics

**Consent**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Help Improve NM Script Editor                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ We'd like to collect anonymous usage data to  â”‚
â”‚ improve the editor. No personal information   â”‚
â”‚ or script content is collected.               â”‚
â”‚                                               â”‚
â”‚ Data collected:                               â”‚
â”‚  â€¢ Feature usage (e.g., Command Palette opens)â”‚
â”‚  â€¢ Error rates (types and frequency)          â”‚
â”‚  â€¢ Performance metrics (load times)           â”‚
â”‚                                               â”‚
â”‚ You can change this in Settings at any time.  â”‚
â”‚                                               â”‚
â”‚ [Learn More]  [Decline]  [Enable Analytics]   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Data Retention**: 90 days rolling window
**User Rights**: Opt-out anytime, data deletion on request

### 7.5 Dashboard & Reporting

**Real-Time Dashboard** (for developers):
- Feature adoption rates over time
- Error rate trends
- Performance regression alerts
- User segment breakdown

**Deliverable**: Analytics Dashboard (Ongoing)
- Weekly usage reports
- Monthly trend analysis
- Quarterly comparison with survey results

---

## 8. Data Analysis & Triangulation

### 8.1 Cross-Method Validation

Triangulate findings across all 4 phases:

| Finding | Interviews | Usability Tests | Survey | Analytics |
|---------|-----------|----------------|--------|-----------|
| Command Palette undiscoverable | âœ“ (5/10 didn't know) | âœ“ (40% completion) | âœ“ (35% aware) | âœ“ (Low usage) |
| Error messages unclear | âœ“ (7/10 mentioned) | âœ“ (Task 3 struggles) | âœ“ (Open-ended) | âœ“ (High resolution time) |
| Live preview desired | âœ“ (8/10 requested) | N/A | âœ“ (75% want it) | N/A |

**Validation Criteria**:
- **Confirmed**: Finding appears in 3+ methods
- **Likely**: Finding appears in 2 methods
- **Uncertain**: Finding appears in 1 method only
- **Refuted**: Methods contradict each other

### 8.2 Mapping to Technical Analysis

For each of the 42 technical issues (#229-#247 + extensions), determine:
1. **Validated**: Users experience this issue
2. **Not Validated**: Users don't mention this issue
3. **Severity Revised**: Users rate it higher/lower severity than expected

Example mapping:

| Issue # | Technical Issue | User Validation | Severity Change |
|---------|----------------|-----------------|----------------|
| #237 | Toolbar lacks icons | âœ“ Validated | â†‘ Higher (users rely on visual scanning) |
| #238 | No onboarding | âœ“ Validated | â†‘ Higher (beginners quit early) |
| #239 | Graph integration gaps | âœ“ Validated | = Same |
| #240 | No live preview | âœ“ Validated | â†‘ Higher (most requested) |
| #241 | No asset validation | âœ“ Validated | â†‘ Higher (build failures) |

### 8.3 Discovering New Issues

Identify issues **missed by technical analysis**:
- Workflow inefficiencies not visible in code
- Conceptual misunderstandings about system
- Documentation gaps
- Terminology confusion
- Unexpected use cases

---

## 9. Deliverables & Timeline

### 9.1 Deliverables

1. **Interview Summary Report** (Week 2)
   - Thematic analysis of pain points
   - Key quotes and insights
   - Workflow diagrams
   - PDF: ~15-20 pages

2. **Usability Test Report** (Week 4)
   - Task completion statistics
   - Time-on-task analysis
   - Heatmaps and video highlights
   - Top 10 friction points
   - PDF: ~25-30 pages

3. **Survey Analysis Report** (Week 5)
   - Response rate and demographics
   - Feature awareness dashboard (charts)
   - NPS and satisfaction scores
   - Open-ended feedback summary
   - PDF: ~15-20 pages

4. **UX Improvement Roadmap** (Week 6)
   - Prioritized issue list (updated from #227)
   - Severity re-ranking based on user data
   - Quick wins vs. long-term fixes
   - Effort-Impact matrix
   - PDF: ~10-15 pages

5. **Analytics Dashboard** (Ongoing)
   - Live dashboard link (read-only)
   - Weekly usage reports (email)
   - Quarterly trend reports

6. **Raw Data Archive**
   - Anonymized interview transcripts
   - Usability test recordings (with consent)
   - Survey responses (CSV)
   - Analytics data dumps

### 9.2 Timeline

```
Week 1: Interviews (5 participants)
Week 2: Interviews (5-10 participants) + Report
Week 3: Usability Tests (8 participants)
Week 4: Usability Tests (7-12 participants) + Report
Week 5: Survey launch + 1 week collection + Analysis
Week 6: Final report integration + Roadmap
Week 7+: Analytics continuous monitoring
```

**Milestones**:
- âœ… Week 2: Interview Report delivered
- âœ… Week 4: Usability Test Report delivered
- âœ… Week 5: Survey Analysis delivered
- âœ… Week 6: UX Roadmap delivered + **Study Complete**

---

## 10. Success Metrics

### 10.1 Study Quality Metrics

**Recruitment Success**:
- âœ… 10+ interview participants
- âœ… 15+ usability test participants
- âœ… 50+ survey responses
- âœ… 30%+ analytics opt-in rate

**Data Quality**:
- âœ… 90%+ interview completion rate
- âœ… 80%+ usability task completion rate
- âœ… 75%+ survey completion rate (no partial responses)
- âœ… <10% analytics opt-out after initial opt-in

### 10.2 Usability Targets

**Baseline Targets** (from issue #247):
- âœ… Task completion rate: 80%+
- âœ… Satisfaction: Average 4.0+ / 5.0
- âœ… Efficiency: 20%+ faster task completion after fixes (measured in future study)
- âœ… Discoverability: 60%+ aware of key features (Command Palette, Snippets)

### 10.3 Outcome Metrics

**Validation Rate**:
- âœ… 70%+ of technical issues validated by users
- âœ… 10+ new issues discovered by user research
- âœ… Clear priority ranking for top 20 issues

**Actionability**:
- âœ… 100% of high-priority issues have concrete user evidence
- âœ… Roadmap includes effort estimates and impact scores
- âœ… Quick wins identified (high impact, low effort)

---

## 11. Ethical Considerations

### 11.1 Informed Consent

All participants will:
- Receive plain-language study description
- Understand data usage and anonymization
- Provide explicit consent (checkboxes, not implied)
- Be informed of right to withdraw anytime

### 11.2 Privacy Protection

**Interviews & Usability Tests**:
- No real names in reports (Participant 1, 2, 3...)
- No identifiable project details shared publicly
- Recordings stored securely, deleted after 6 months
- Transcripts anonymized

**Survey**:
- No email/IP collection unless participant opts in for follow-up
- Responses aggregated, not individually identifiable

**Analytics**:
- No PII collected (no usernames, emails, IP addresses)
- Device fingerprinting prohibited
- Local aggregation before cloud upload
- User can inspect data before sending (transparency)

### 11.3 Compensation Equity

- **All participants compensated equally** within each phase
- **No penalty for incomplete tasks** (usability tests)
- **Flexible scheduling** for participants in different time zones
- **Alternative formats** for accessibility needs

---

## 12. Limitations & Mitigation

### 12.1 Sampling Bias

**Risk**: Volunteers may not represent typical users
**Mitigation**:
- Stratified sampling by experience level
- Recruit from multiple channels (not just power users on GitHub)
- Compare analytics (all users) with survey (volunteers) to detect bias

### 12.2 Hawthorne Effect

**Risk**: Participants behave differently when observed
**Mitigation**:
- Emphasize "no right/wrong answers" in usability tests
- Allow warmup period before recording
- Mix observed (usability tests) with unobserved (analytics) methods

### 12.3 Small Sample Size

**Risk**: Interview/usability test N=25-35 may miss rare issues
**Mitigation**:
- Supplement with large-scale survey (N=50+)
- Use analytics for statistical power
- Iterate study if critical gaps found

### 12.4 Self-Selection Bias (Analytics Opt-In)

**Risk**: Only power users opt into analytics
**Mitigation**:
- Make opt-in prominent but not nagging
- Emphasize benefits ("help improve")
- Analyze opt-in demographics for bias

---

## 13. Budget Estimate (Optional)

### 13.1 Participant Incentives

| Phase | Participants | Incentive | Total |
|-------|-------------|-----------|-------|
| Interviews | 10-15 | $50 | $500-750 |
| Usability Tests | 15-20 | $30 | $450-600 |
| Survey Raffle | 50+ | $100 prize | $100 |
| **Total** | | | **$1,050-1,450** |

### 13.2 Tools & Infrastructure

| Item | Cost |
|------|------|
| Zoom/Google Meet | Free (existing) |
| Survey platform (Qualtrics) | Free tier or $100/mo |
| Analytics server (cloud hosting) | $20/mo x 3 = $60 |
| Screen recording (Loom) | Free tier |
| **Total** | **~$160-360** |

### 13.3 Personnel Time (If External)

| Role | Hours | Rate | Total |
|------|-------|------|-------|
| UX Researcher | 120h | $75/h | $9,000 |
| Data Analyst | 40h | $60/h | $2,400 |
| Report Writing | 30h | $75/h | $2,250 |
| **Total** | 190h | | **$13,650** |

**Note**: This study plan can be executed by contributors/volunteers, reducing cost to just incentives + tools (~$1,200-1,800).

---

## 14. Next Steps

### 14.1 Study Launch Checklist

- [ ] **Approve Study Protocol**: Review with stakeholders
- [ ] **Finalize Budget**: Allocate funds for incentives
- [ ] **Prepare Recruitment Materials**: Posters, emails, social posts
- [ ] **Set Up Tools**: Zoom, survey platform, recording software
- [ ] **Create Consent Forms**: Legal review if needed
- [ ] **Pilot Test Protocols**: Run 1-2 practice interviews/usability tests
- [ ] **Launch Recruitment**: Post on GitHub, Discord, social media
- [ ] **Schedule Participants**: Use Calendly or similar
- [ ] **Begin Data Collection**: Week 1 interviews

### 14.2 Post-Study Actions

After Week 6:
- [ ] **Present Findings**: Share reports with community
- [ ] **Update Issue Priorities**: Re-rank #229-#247 based on data
- [ ] **Create Implementation Plan**: Assign issues to dev roadmap
- [ ] **Publish Summary**: Blog post / GitHub discussion
- [ ] **Archive Data**: Securely store and back up raw data
- [ ] **Plan Follow-Up**: Schedule repeat study in 6-12 months to measure improvement

---

## 15. References & Resources

### 15.1 UX Research Methodologies

1. **Nielsen, J.** (1994). *Usability Engineering*. Morgan Kaufmann.
2. **Krug, S.** (2014). *Don't Make Me Think, Revisited: A Common Sense Approach to Web Usability*.
3. **Goodman, E., Kuniavsky, M., & Moed, A.** (2012). *Observing the User Experience: A Practitioner's Guide*.

### 15.2 Related Studies

- **Ren'Py User Survey 2023**: Similar study on VN tool usability
- **VS Code UX Research**: Microsoft's approach to editor usability
- **Unity Editor Usability Studies**: Game engine UX best practices

### 15.3 Analysis Tools

- **Thematic Analysis**: Braun & Clarke (2006) method
- **System Usability Scale (SUS)**: Standardized 10-item questionnaire
- **Net Promoter Score (NPS)**: Loyalty metric calculation

---

## 16. Conclusion

This comprehensive UX study will provide **empirical validation** of the technical analysis conducted in issue #227, while also **discovering blind spots** missed by code inspection alone. By combining qualitative depth (interviews), quantitative rigor (usability tests), broad reach (survey), and continuous monitoring (analytics), we ensure triangulated, actionable insights.

**Expected Impact**:
- âœ… Validate 70%+ of 42 technical issues
- âœ… Discover 10+ new issues missed by analysis
- âœ… Create evidence-based priority ranking
- âœ… Establish baseline metrics for future comparison
- âœ… Improve NM Script Editor usability from **7.5/10 â†’ 9/10** after implementing fixes

**Timeline**: 6 weeks + ongoing analytics
**Budget**: $1,200-1,800 (if using volunteers) or $15,000 (if external researchers)
**Risk**: Low (ethical protocols, validated methods)
**Benefit**: High (directly informs development priorities)

---

**Prepared by**: AI Issue Solver
**Date**: 2026-01-08
**Status**: Ready for approval and execution
**Contact**: Issue #247 discussion thread

---

## Appendix A: Sample Consent Form

### Research Study Consent Form

**Study Title**: NM Script Editor User Experience Study
**Principal Investigator**: [Name], NovelMind Project

#### Purpose
This study aims to understand how users interact with the NM Script Editor to identify areas for improvement.

#### Procedures
You will be asked to:
- (Interviews) Answer questions about your experience (45-60 minutes)
- (Usability Tests) Complete tasks while thinking aloud (45 minutes)
- (Survey) Answer a questionnaire (5-7 minutes)
- (Analytics) Opt-in to anonymous usage tracking (passive)

#### Risks & Benefits
- **Risks**: None anticipated. Minor frustration if tasks are difficult.
- **Benefits**: Contribute to improving the editor. Receive gift card compensation (where applicable).

#### Confidentiality
- Your identity will not be shared publicly
- Data will be anonymized in reports
- Recordings will be deleted after 6 months
- You can request data deletion at any time

#### Voluntary Participation
- You may withdraw at any time without penalty
- You may skip any questions
- Compensation will not be affected by withdrawal

#### Contact
Questions? Contact [email] or open an issue at [GitHub URL]

**I consent to participate in this study:**
- [ ] Yes, I consent
- [ ] No, I decline

**I consent to audio/video recording (interviews/usability tests only):**
- [ ] Yes, I consent to recording
- [ ] No, I do not consent to recording (notes only)

Signature: _________________ Date: _________

---

## Appendix B: Usability Test Script Template

### Pre-Test

"Thank you for participating! This session will take about 45 minutes. I'll ask you to complete some tasks in the NM Script Editor while thinking aloudâ€”tell me what you're thinking, what you expect to happen, and what confuses you.

There are no right or wrong answers, and you're not being testedâ€”the editor is being tested. If you get stuck, that's valuable feedback. Feel free to ask questions, but I might not be able to answer them right away since I want to see how you'd solve problems on your own.

Do you have any questions before we start? I'm going to start recording nowâ€”is that OK?"

[Start recording]

### Task Introduction Template

"OK, here's your next task: [read scenario]. Take your time, and please think aloud as you work. Let me know when you think you're done."

[Observe silently. Only prompt if silent for >15 seconds: "What are you thinking?"]

### Post-Task

"On a scale of 1 to 5, how difficult was that task?"
- 1 = Very easy
- 2 = Easy
- 3 = Neutral
- 4 = Difficult
- 5 = Very difficult

"What made it easy/difficult?"

### Post-Test

"That's all the tasks! A few final questions:
- Overall, how satisfied are you with the editor? (1-5)
- What was the most frustrating part of using it today?
- What did you like most about it?
- Any other feedback?"

"Thank you so much! Your feedback is incredibly valuable. You'll receive your [gift card] within [timeframe]. If you have any questions later, here's my contact info."

---

## Appendix C: Survey Distribution Template

### GitHub Issue Comment

```markdown
## ğŸ“Š User Experience Study - We Need Your Input!

We're conducting a comprehensive UX study to improve the NM Script Editor based on YOUR feedback.

**What**: 5-minute survey about your experience with the editor
**Why**: Your input will directly shape our development priorities
**Incentive**: Enter to win a $100 gift card!

ğŸ‘‰ **[Take the Survey](link)**

We're also looking for participants for 1-on-1 interviews ($50 gift card) and usability testing sessions ($30 gift card). Interested? Indicate in the survey.

Your feedback mattersâ€”thank you! ğŸ™
```

### Discord / Forum Post

```markdown
Hey everyone! ğŸ‘‹

We're running a **UX study** to make NM Script Editor even better, and we need your help!

ğŸ“ **Quick 5-minute survey** â†’ [Link]
ğŸ Enter to win $100 gift card

We're also looking for folks to:
- ğŸ’¬ Participate in interviews (45 min, $50 gift card)
- ğŸ–±ï¸ Do usability testing (45 min, $30 gift card)

All experience levels welcomeâ€”beginners especially needed!

Questions? Reply here or DM me. Thanks! ğŸš€
```

---

## Appendix D: Analytics Telemetry Specification

### Data Schema (JSON)

```json
{
  "session_id": "uuid",
  "timestamp": "ISO8601",
  "editor_version": "v0.2.0",
  "os": "Windows|Linux|macOS",
  "user_segment": "beginner|intermediate|advanced",

  "events": [
    {
      "type": "command_palette_open",
      "timestamp": "ISO8601",
      "duration_ms": 1234,
      "query": "hashed_query",
      "selected_command": "format_document"
    },
    {
      "type": "snippet_insert",
      "timestamp": "ISO8601",
      "snippet_name": "show_character",
      "method": "keyboard_shortcut|menu"
    },
    {
      "type": "syntax_error",
      "timestamp": "ISO8601",
      "error_code": "E001",
      "resolution_time_ms": 45000,
      "resolved": true
    }
  ],

  "session_summary": {
    "duration_ms": 3600000,
    "files_opened": 5,
    "lines_written": 120,
    "features_used": ["command_palette", "go_to_definition"],
    "errors_encountered": 3,
    "memory_peak_mb": 256
  }
}
```

### Privacy Compliance

- **No PII**: No names, emails, IP addresses
- **No Content**: No script text, file names, or project names
- **Opt-In**: Explicit consent required
- **Transparent**: User can view data before sending
- **Deletable**: User can request data deletion
- **Minimal**: Only essential metrics collected

---

**End of Document**

---

*This UX research plan is a living document and may be updated based on stakeholder feedback and pilot test results.*
